{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Quality Dataset - Feature Engineering Pipeline\n",
    "\n",
    "This notebook provides a complete feature engineering pipeline for the code quality dataset:\n",
    "- Data extraction and JSON parsing\n",
    "- Feature engineering and transformation\n",
    "- Missing value handling and outlier detection\n",
    "- Normalization and encoding\n",
    "- Production-ready dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import ast\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Definition: CodeQualityFeatureBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CodeQualityFeatureBuilder class defined\n"
     ]
    }
   ],
   "source": [
    "class CodeQualityFeatureBuilder:\n",
    "    \"\"\"Build and engineer features from raw code quality dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_csv: str, output_csv: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the feature builder.\n",
    "        \n",
    "        Args:\n",
    "            input_csv: Path to input CSV file\n",
    "            output_csv: Path to output CSV file (default: input_csv_processed.csv)\n",
    "        \"\"\"\n",
    "        self.input_csv = input_csv\n",
    "        self.output_csv = output_csv or input_csv.replace('.csv', '_processed.csv')\n",
    "        self.df = None\n",
    "        self.original_shape = None\n",
    "        \n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load CSV data with error handling.\"\"\"\n",
    "        logger.info(f\"Loading data from {self.input_csv}\")\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.input_csv, low_memory=False)\n",
    "            self.original_shape = self.df.shape\n",
    "            logger.info(f\"Data loaded successfully. Shape: {self.original_shape}\")\n",
    "            return self.df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def identify_data_types(self) -> Dict[str, str]:\n",
    "        \"\"\"Identify and categorize columns by type.\"\"\"\n",
    "        type_mapping = {}\n",
    "        \n",
    "        for col in self.df.columns:\n",
    "            if col in ['y_binary', 'unit_test_presence', 'vcs_available']:\n",
    "                type_mapping[col] = 'boolean'\n",
    "            elif col in ['file_path', 'vcs_top_coupled']:\n",
    "                type_mapping[col] = 'categorical'\n",
    "            elif col in ['coupled_file_changes', 'cross_file_call_edges', 'smells', 'pep8_examples']:\n",
    "                type_mapping[col] = 'json_dict'\n",
    "            elif self.df[col].dtype == 'object':\n",
    "                type_mapping[col] = 'object'\n",
    "            else:\n",
    "                type_mapping[col] = 'numeric'\n",
    "        \n",
    "        return type_mapping\n",
    "    \n",
    "    def parse_json_columns(self):\n",
    "        \"\"\"Parse JSON/dict-like columns safely.\"\"\"\n",
    "        json_like_cols = ['coupled_file_changes', 'cross_file_call_edges', 'smells', \n",
    "                         'pep8_examples', 'indentation_irregularity', 'god_class_proxies', 'pep8_violations']\n",
    "        \n",
    "        for col in json_like_cols:\n",
    "            if col in self.df.columns:\n",
    "                logger.info(f\"Parsing JSON column: {col}\")\n",
    "                self.df[col] = self.df[col].apply(self._safe_parse_json)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _safe_parse_json(val: Any) -> Any:\n",
    "        \"\"\"Safely parse JSON/dict strings.\"\"\"\n",
    "        if pd.isna(val) or val == '':\n",
    "            return None\n",
    "        if isinstance(val, (int, float)):\n",
    "            return val\n",
    "        if isinstance(val, (dict, list)):\n",
    "            return val\n",
    "        try:\n",
    "            return json.loads(val)\n",
    "        except (json.JSONDecodeError, ValueError):\n",
    "            try:\n",
    "                return ast.literal_eval(str(val))\n",
    "            except (ValueError, SyntaxError):\n",
    "                return None\n",
    "    \n",
    "    def handle_missing_values(self):\n",
    "        \"\"\"Handle missing and invalid values.\"\"\"\n",
    "        logger.info(\"Handling missing values...\")\n",
    "        \n",
    "        # Count missing values\n",
    "        missing_counts = self.df.isnull().sum()\n",
    "        if missing_counts.sum() > 0:\n",
    "            logger.info(f\"Missing values found:\\n{missing_counts[missing_counts > 0]}\")\n",
    "        \n",
    "        # Fill numeric columns with median\n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            if self.df[col].isnull().sum() > 0:\n",
    "                median_val = self.df[col].median()\n",
    "                self.df[col].fillna(median_val, inplace=True)\n",
    "                logger.info(f\"Filled {col} with median: {median_val}\")\n",
    "        \n",
    "        # Fill categorical columns with 'Unknown'\n",
    "        categorical_cols = self.df.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_cols:\n",
    "            if self.df[col].isnull().sum() > 0:\n",
    "                self.df[col].fillna('Unknown', inplace=True)\n",
    "        \n",
    "        # Fill boolean columns with False\n",
    "        boolean_cols = self.df.select_dtypes(include=['bool']).columns\n",
    "        for col in boolean_cols:\n",
    "            if self.df[col].isnull().sum() > 0:\n",
    "                self.df[col].fillna(False, inplace=True)\n",
    "    \n",
    "    def create_derived_features(self):\n",
    "        \"\"\"Engineer new features from existing ones.\"\"\"\n",
    "        logger.info(\"Creating derived features...\")\n",
    "        \n",
    "        # Code quality complexity score\n",
    "        complexity_cols = ['average_cyclomatic_complexity', 'max_cyclomatic_ratio', 'mean_cyclomatic_ratio']\n",
    "        self.df['complexity_score'] = self.df[complexity_cols].mean(axis=1)\n",
    "        \n",
    "        # Code health indicator\n",
    "        self.df['code_health'] = (\n",
    "            (100 - self.df['pep8_violations'].fillna(0)) * 0.3 +\n",
    "            self.df['maintainability_score'].fillna(50) * 0.4 +\n",
    "            (100 - self.df['comment_code_mismatch_score'].fillna(0) * 100) * 0.3\n",
    "        )\n",
    "        \n",
    "        # Documentation quality\n",
    "        self.df['doc_quality'] = (\n",
    "            self.df['documentation_coverage'].fillna(0) * 0.5 +\n",
    "            (100 - self.df['comment_percentage'].fillna(0)) * 0.5\n",
    "        )\n",
    "        \n",
    "        # Testing coverage indicator\n",
    "        if 'test_to_source_ratio' in self.df.columns:\n",
    "            self.df['has_tests'] = (self.df['test_to_source_ratio'].fillna(0) > 0).astype(int)\n",
    "        \n",
    "        # Coupling complexity\n",
    "        if 'inter_file_coupling' in self.df.columns and 'call_graph_density' in self.df.columns:\n",
    "            self.df['coupling_complexity'] = (\n",
    "                self.df['inter_file_coupling'].fillna(0) * 0.5 +\n",
    "                self.df['call_graph_density'].fillna(0) * 0.5\n",
    "            )\n",
    "        \n",
    "        # Code smell density\n",
    "        self.df['smell_density'] = self.df['smells'].apply(\n",
    "            lambda x: len(x) if isinstance(x, list) else 0\n",
    "        ) / (self.df['lines_of_code'].fillna(1) / 100)\n",
    "        \n",
    "        # Effort-to-impact ratio\n",
    "        self.df['effort_impact_ratio'] = (\n",
    "            self.df['halstead_effort'].fillna(0) / \n",
    "            (self.df['halstead_estimated_bugs'].fillna(1) + 1)\n",
    "        )\n",
    "        \n",
    "        # File maturity (based on age and changes)\n",
    "        self.df['file_maturity'] = np.log1p(self.df['file_age_days'].fillna(0)) * \\\n",
    "                                   (1 + self.df['lines_added'].fillna(0) / \n",
    "                                   (self.df['lines_of_code'].fillna(1)))\n",
    "        \n",
    "        logger.info(f\"Created 8 new features\")\n",
    "    \n",
    "    def encode_categorical_features(self):\n",
    "        \"\"\"Encode categorical columns.\"\"\"\n",
    "        logger.info(\"Encoding categorical features...\")\n",
    "        \n",
    "        # Binary encoding for boolean columns\n",
    "        boolean_cols = ['unit_test_presence', 'vcs_available']\n",
    "        for col in boolean_cols:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = self.df[col].astype(int)\n",
    "        \n",
    "        # One-hot encode file_path directory (top-level project)\n",
    "        if 'file_path' in self.df.columns:\n",
    "            self.df['project'] = self.df['file_path'].apply(\n",
    "                lambda x: str(x).split('\\\\')[5] if isinstance(x, str) and len(str(x).split('\\\\')) > 5 else 'unknown'\n",
    "            )\n",
    "            # Keep top 10 projects, rest as 'other'\n",
    "            top_projects = self.df['project'].value_counts().head(10).index\n",
    "            self.df['project'] = self.df['project'].apply(\n",
    "                lambda x: x if x in top_projects else 'other'\n",
    "            )\n",
    "            self.df = pd.get_dummies(self.df, columns=['project'], drop_first=True)\n",
    "    \n",
    "    def handle_outliers(self):\n",
    "        \"\"\"Detect and handle outliers using IQR method.\"\"\"\n",
    "        logger.info(\"Handling outliers...\")\n",
    "        \n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        outliers_found = {}\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col.startswith('project_'):  # Skip one-hot encoded columns\n",
    "                continue\n",
    "            \n",
    "            Q1 = self.df[col].quantile(0.25)\n",
    "            Q3 = self.df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            lower_bound = Q1 - 3 * IQR\n",
    "            upper_bound = Q3 + 3 * IQR\n",
    "            \n",
    "            outlier_count = ((self.df[col] < lower_bound) | (self.df[col] > upper_bound)).sum()\n",
    "            \n",
    "            if outlier_count > 0:\n",
    "                outliers_found[col] = outlier_count\n",
    "                # Cap outliers instead of removing\n",
    "                self.df[col] = self.df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "        \n",
    "        if outliers_found:\n",
    "            logger.info(f\"Outliers capped: {outliers_found}\")\n",
    "    \n",
    "    def normalize_features(self):\n",
    "        \"\"\"Normalize numeric features to 0-1 scale.\"\"\"\n",
    "        logger.info(\"Normalizing features...\")\n",
    "        \n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        skip_cols = [col for col in numeric_cols if col.startswith('project_')] + \\\n",
    "                   ['y_binary', 'has_tests']  # Don't normalize binary target/flags\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col not in skip_cols:\n",
    "                min_val = self.df[col].min()\n",
    "                max_val = self.df[col].max()\n",
    "                \n",
    "                if max_val > min_val:\n",
    "                    self.df[col] = (self.df[col] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    def remove_low_variance_features(self, threshold: float = 0.01):\n",
    "        \"\"\"Remove features with very low variance.\"\"\"\n",
    "        logger.info(f\"Removing low variance features (threshold: {threshold})...\")\n",
    "        \n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        skip_cols = [col for col in numeric_cols if col.startswith('project_')]\n",
    "        \n",
    "        removed = []\n",
    "        for col in numeric_cols:\n",
    "            if col not in skip_cols:\n",
    "                variance = self.df[col].var()\n",
    "                if variance < threshold:\n",
    "                    removed.append(col)\n",
    "                    self.df.drop(col, axis=1, inplace=True)\n",
    "        \n",
    "        if removed:\n",
    "            logger.info(f\"Removed low variance features: {removed}\")\n",
    "    \n",
    "    def drop_unnecessary_columns(self):\n",
    "        \"\"\"Drop columns that are not useful for modeling.\"\"\"\n",
    "        logger.info(\"Dropping unnecessary columns...\")\n",
    "        \n",
    "        cols_to_drop = []\n",
    "        \n",
    "        # Drop file_path (replaced with project feature)\n",
    "        if 'file_path' in self.df.columns:\n",
    "            cols_to_drop.append('file_path')\n",
    "        \n",
    "        # Drop raw JSON columns (features already extracted)\n",
    "        json_cols = ['coupled_file_changes', 'cross_file_call_edges', 'smells', \n",
    "                    'pep8_examples', 'indentation_irregularity', 'god_class_proxies', \n",
    "                    'pep8_violations', 'vcs_top_coupled']\n",
    "        for col in json_cols:\n",
    "            if col in self.df.columns:\n",
    "                cols_to_drop.append(col)\n",
    "        \n",
    "        # Drop redundant raw columns (we have encoded versions)\n",
    "        redundant_cols = []\n",
    "        if 'project' in self.df.columns:\n",
    "            cols_to_drop.append('project')\n",
    "        \n",
    "        self.df.drop(cols_to_drop, axis=1, inplace=True, errors='ignore')\n",
    "        logger.info(f\"Dropped {len(cols_to_drop)} columns\")\n",
    "    \n",
    "    def validate_final_data(self):\n",
    "        \"\"\"Validate the final dataset.\"\"\"\n",
    "        logger.info(\"Validating final dataset...\")\n",
    "        \n",
    "        # Check for remaining NaN values\n",
    "        nan_count = self.df.isnull().sum().sum()\n",
    "        if nan_count > 0:\n",
    "            logger.warning(f\"Found {nan_count} NaN values remaining\")\n",
    "            # Fill remaining NaNs with 0\n",
    "            self.df.fillna(0, inplace=True)\n",
    "        \n",
    "        # Check for infinite values\n",
    "        inf_count = np.isinf(self.df.select_dtypes(include=[np.number])).sum().sum()\n",
    "        if inf_count > 0:\n",
    "            logger.warning(f\"Found {inf_count} infinite values, replacing with 0\")\n",
    "            self.df = self.df.replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        # Validate target variable\n",
    "        if 'y_binary' in self.df.columns:\n",
    "            unique_targets = self.df['y_binary'].unique()\n",
    "            logger.info(f\"Target variable distribution: {self.df['y_binary'].value_counts().to_dict()}\")\n",
    "        \n",
    "        logger.info(f\"Final dataset shape: {self.df.shape}\")\n",
    "        logger.info(f\"Final columns: {list(self.df.columns)}\")\n",
    "    \n",
    "    def generate_summary_statistics(self):\n",
    "        \"\"\"Generate and log summary statistics.\"\"\"\n",
    "        logger.info(\"\\n\" + \"=\"*60)\n",
    "        logger.info(\"DATASET SUMMARY STATISTICS\")\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(f\"\\nOriginal shape: {self.original_shape}\")\n",
    "        logger.info(f\"Final shape: {self.df.shape}\")\n",
    "        logger.info(f\"\\nNumeric columns: {len(self.df.select_dtypes(include=[np.number]).columns)}\")\n",
    "        logger.info(f\"Categorical columns: {len(self.df.select_dtypes(include=['object']).columns)}\")\n",
    "        logger.info(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    def build(self) -> pd.DataFrame:\n",
    "        \"\"\"Execute the complete feature building pipeline.\"\"\"\n",
    "        logger.info(\"Starting feature engineering pipeline...\")\n",
    "        \n",
    "        # Load data\n",
    "        self.load_data()\n",
    "        \n",
    "        # Parse JSON columns\n",
    "        self.parse_json_columns()\n",
    "        \n",
    "        # Handle missing values\n",
    "        self.handle_missing_values()\n",
    "        \n",
    "        # Create derived features\n",
    "        self.create_derived_features()\n",
    "        \n",
    "        # Encode categorical features\n",
    "        self.encode_categorical_features()\n",
    "        \n",
    "        # Handle outliers\n",
    "        self.handle_outliers()\n",
    "        \n",
    "        # Remove low variance features\n",
    "        self.remove_low_variance_features()\n",
    "        \n",
    "        # Drop unnecessary columns\n",
    "        self.drop_unnecessary_columns()\n",
    "        \n",
    "        # Normalize features\n",
    "        self.normalize_features()\n",
    "        \n",
    "        # Validate final data\n",
    "        self.validate_final_data()\n",
    "        \n",
    "        # Generate summary\n",
    "        self.generate_summary_statistics()\n",
    "        \n",
    "        logger.info(f\"Pipeline complete! Saving to {self.output_csv}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save processed data to CSV.\"\"\"\n",
    "        try:\n",
    "            self.df.to_csv(self.output_csv, index=False)\n",
    "            logger.info(f\"Data successfully saved to {self.output_csv}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving data: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "print(\"✓ CodeQualityFeatureBuilder class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file: ../../data/processed/dataset.csv\n",
      "Output file: ../../data/processed/dataset_processed.csv\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Adjust these paths as needed\n",
    "input_file = \"../../data/processed/dataset.csv\"  # Input CSV file\n",
    "output_file = \"../../data/processed/dataset_processed.csv\"  # Output file\n",
    "\n",
    "print(f\"Input file: {input_file}\")\n",
    "print(f\"Output file: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:19:58,772 - INFO - Starting feature engineering pipeline...\n",
      "2025-12-26 01:19:58,774 - INFO - Loading data from ../../data/processed/dataset.csv\n",
      "2025-12-26 01:19:58,850 - INFO - Data loaded successfully. Shape: (3088, 67)\n",
      "2025-12-26 01:19:58,851 - INFO - Parsing JSON column: coupled_file_changes\n",
      "2025-12-26 01:19:58,859 - INFO - Parsing JSON column: cross_file_call_edges\n",
      "2025-12-26 01:19:59,007 - INFO - Parsing JSON column: smells\n",
      "2025-12-26 01:19:59,110 - INFO - Parsing JSON column: pep8_examples\n",
      "2025-12-26 01:19:59,137 - INFO - Parsing JSON column: indentation_irregularity\n",
      "2025-12-26 01:19:59,194 - INFO - Parsing JSON column: god_class_proxies\n",
      "2025-12-26 01:19:59,200 - INFO - Parsing JSON column: pep8_violations\n",
      "2025-12-26 01:19:59,203 - INFO - Handling missing values...\n",
      "2025-12-26 01:19:59,212 - INFO - Creating derived features...\n",
      "2025-12-26 01:19:59,217 - INFO - Created 8 new features\n",
      "2025-12-26 01:19:59,218 - INFO - Encoding categorical features...\n",
      "2025-12-26 01:19:59,229 - INFO - Handling outliers...\n",
      "2025-12-26 01:19:59,297 - INFO - Outliers capped: {'attribute_mutations_outside_init': np.int64(106), 'average_cyclomatic_complexity': np.int64(24), 'average_methods_per_class': np.int64(363), 'avg_line_length': np.int64(4), 'boolean_expression_avg_terms': np.int64(646), 'call_graph_density': np.int64(733), 'classes': np.int64(731), 'classes_with_inheritance': np.int64(491), 'comment_lines': np.int64(282), 'comment_percentage': np.int64(306), 'commit_bursts': np.int64(16), 'external_vs_internal_field_access_ratio': np.int64(193), 'file_age_days': np.int64(12), 'functions': np.int64(114), 'global_usages_total': np.int64(316), 'globals_declared': np.int64(67), 'halstead_difficulty': np.int64(95), 'halstead_effort': np.int64(336), 'halstead_estimated_bugs': np.int64(226), 'halstead_volume': np.int64(226), 'inter_file_coupling': np.int64(77), 'lines_added': np.int64(142), 'lines_deleted': np.int64(19), 'lines_of_code': np.int64(142), 'max_intra_file_call_depth': np.int64(710), 'max_line_length': np.int64(379), 'max_lines_per_function': np.int64(60), 'max_nesting_level': np.int64(4), 'mean_lines_per_function': np.int64(25), 'methods': np.int64(363), 'nesting_variance': np.int64(35), 'num_authors': np.int64(13), 'pep8_violations': np.int64(186), 'percent_lines_over_80': np.int64(46), 'semantic_todo_density': np.int64(130), 'source_lines': np.int64(202), 'test_files_found': np.int64(426), 'test_function_count': np.int64(511), 'test_lines': np.int64(482), 'test_to_source_ratio': np.int64(553), 'todo_fixme_count': np.int64(130), 'todo_fixme_semantic_density': np.int64(130), 'total_imports': np.int64(55), 'unit_test_presence': np.int64(664), 'complexity_score': np.int64(27), 'code_health': np.int64(3), 'coupling_complexity': np.int64(85), 'smell_density': np.int64(27), 'effort_impact_ratio': np.int64(309), 'file_maturity': np.int64(12)}\n",
      "2025-12-26 01:19:59,298 - INFO - Removing low variance features (threshold: 0.01)...\n",
      "2025-12-26 01:19:59,330 - INFO - Removed low variance features: ['attribute_mutations_outside_init', 'average_methods_per_class', 'boolean_expression_avg_terms', 'call_graph_density', 'classes', 'classes_with_inheritance', 'commit_bursts', 'file_age_days', 'lines_deleted', 'max_intra_file_call_depth', 'max_lines_per_class', 'mean_lines_per_class', 'methods', 'num_authors', 'semantic_todo_density', 'todo_fixme_count', 'todo_fixme_semantic_density', 'unit_test_presence', 'vcs_available', 'file_maturity']\n",
      "2025-12-26 01:19:59,330 - INFO - Dropping unnecessary columns...\n",
      "2025-12-26 01:19:59,335 - INFO - Dropped 9 columns\n",
      "2025-12-26 01:19:59,336 - INFO - Normalizing features...\n",
      "2025-12-26 01:19:59,344 - INFO - Validating final dataset...\n",
      "2025-12-26 01:19:59,351 - INFO - Target variable distribution: {'[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]': 403, '[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]': 175, '[0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0]': 149, '[0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]': 97, '[0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]': 89, '[0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0]': 89, '[1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0]': 83, '[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0]': 75, '[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]': 70, '[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]': 63, '[1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0]': 61, '[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]': 57, '[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]': 50, '[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]': 49, '[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]': 46, '[0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0]': 43, '[1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]': 43, '[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]': 43, '[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]': 41, '[0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0]': 41, '[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]': 41, '[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]': 38, '[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]': 33, '[1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0]': 31, '[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]': 29, '[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0]': 27, '[0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0]': 27, '[0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]': 26, '[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]': 26, '[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]': 24, '[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]': 23, '[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]': 21, '[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]': 18, '[1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0]': 17, '[1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0]': 17, '[1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0]': 17, '[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]': 17, '[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]': 17, '[0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0]': 17, '[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]': 16, '[1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0]': 16, '[1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]': 16, '[0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0]': 16, '[1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0]': 15, '[0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0]': 15, '[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]': 15, '[1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0]': 15, '[1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0]': 15, '[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]': 14, '[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0]': 14, '[0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0]': 14, '[0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0]': 13, '[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0]': 13, '[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0]': 13, '[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0]': 12, '[0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0]': 12, '[1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0]': 12, '[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]': 11, '[1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0]': 11, '[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0]': 11, '[0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0]': 11, '[0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]': 11, '[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]': 11, '[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]': 10, '[1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0]': 9, '[0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0]': 9, '[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0]': 8, '[1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]': 8, '[1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0]': 8, '[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0]': 8, '[1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0]': 7, '[0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0]': 7, '[1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0]': 7, '[0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0]': 7, '[0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0]': 7, '[0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0]': 7, '[0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0]': 6, '[1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0]': 6, '[0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0]': 6, '[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0]': 6, '[1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0]': 6, '[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]': 6, '[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]': 6, '[0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0]': 6, '[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]': 5, '[0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0]': 5, '[1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0]': 5, '[0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0]': 5, '[0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0]': 5, '[1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1]': 5, '[0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]': 5, '[1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0]': 5, '[0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0]': 5, '[1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]': 4, '[0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0]': 4, '[1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1]': 4, '[1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1]': 4, '[1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0]': 4, '[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]': 4, '[1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0]': 4, '[1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0]': 4, '[1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]': 4, '[0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0]': 4, '[1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1]': 4, '[1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0]': 4, '[1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0]': 4, '[0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0]': 4, '[0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0]': 4, '[1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0]': 4, '[1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0]': 3, '[1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0]': 3, '[1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0]': 3, '[1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0]': 3, '[1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0]': 3, '[1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0]': 3, '[0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0]': 3, '[0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0]': 3, '[0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]': 3, '[0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0]': 3, '[1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0]': 3, '[0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0]': 3, '[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0]': 3, '[0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0]': 3, '[1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0]': 3, '[1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1]': 3, '[0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0]': 3, '[1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0]': 3, '[1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0]': 3, '[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]': 3, '[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]': 3, '[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]': 2, '[0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]': 2, '[0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0]': 2, '[0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1]': 2, '[1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0]': 2, '[1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1]': 2, '[0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0]': 2, '[1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]': 2, '[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]': 2, '[1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0]': 2, '[1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0]': 2, '[1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1]': 2, '[1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0]': 2, '[1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0]': 2, '[1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0]': 2, '[1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0]': 2, '[0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0]': 2, '[0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0]': 2, '[1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0]': 2, '[1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0]': 2, '[1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0]': 2, '[1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0]': 2, '[0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0]': 2, '[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0]': 2, '[1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0]': 2, '[1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0]': 2, '[0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0]': 2, '[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]': 2, '[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]': 2, '[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]': 2, '[1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0]': 2, '[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0]': 2, '[1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1]': 2, '[1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1]': 2, '[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0]': 2, '[0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0]': 2, '[0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0]': 2, '[1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0]': 2, '[1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0]': 2, '[0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0]': 2, '[0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1]': 2, '[1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1]': 2, '[0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0]': 2, '[0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0]': 2, '[1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0]': 2, '[1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0]': 2, '[0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1]': 2, '[1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0]': 1, '[1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1]': 1, '[1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0]': 1, '[1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0]': 1, '[1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0]': 1, '[1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0]': 1, '[0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0]': 1, '[1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0]': 1, '[1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0]': 1, '[0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0]': 1, '[1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0]': 1, '[1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0]': 1, '[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0]': 1, '[1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]': 1, '[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0]': 1, '[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0]': 1, '[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]': 1, '[1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]': 1, '[1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0]': 1, '[0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0]': 1, '[1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1]': 1, '[0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0]': 1, '[1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0]': 1, '[1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1]': 1, '[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]': 1, '[1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0]': 1, '[0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1]': 1, '[1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]': 1, '[0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0]': 1, '[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]': 1, '[1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]': 1, '[0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0]': 1, '[1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1]': 1, '[0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0]': 1, '[0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0]': 1, '[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0]': 1, '[0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0]': 1, '[0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0]': 1, '[1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0]': 1, '[1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0]': 1, '[0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0]': 1, '[0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0]': 1, '[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0]': 1, '[1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0]': 1, '[1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1]': 1, '[1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]': 1, '[1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1]': 1, '[0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0]': 1, '[1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0]': 1, '[1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0]': 1, '[1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1]': 1, '[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]': 1, '[1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1]': 1, '[1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0]': 1, '[0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]': 1, '[1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1]': 1, '[1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0]': 1, '[1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0]': 1, '[1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0]': 1, '[1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1]': 1, '[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0]': 1, '[0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0]': 1, '[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0]': 1, '[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0]': 1, '[1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1]': 1, '[0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0]': 1, '[1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0]': 1, '[1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0]': 1, '[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]': 1, '[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]': 1, '[0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0]': 1, '[1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0]': 1, '[1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0]': 1, '[1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0]': 1, '[1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0]': 1, '[0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0]': 1, '[1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1]': 1, '[1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0]': 1, '[1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1]': 1, '[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1]': 1, '[0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0]': 1, '[1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0]': 1, '[1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0]': 1, '[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0]': 1, '[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0]': 1, '[1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0]': 1, '[1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0]': 1, '[1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0]': 1, '[1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1]': 1, '[0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0]': 1, '[1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1]': 1, '[1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0]': 1, '[1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1]': 1, '[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]': 1, '[0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]': 1, '[1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1]': 1, '[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0]': 1, '[1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0]': 1, '[1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0]': 1, '[1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]': 1, '[1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0]': 1, '[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0]': 1, '[0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0]': 1, '[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]': 1, '[1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0]': 1, '[0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0]': 1, '[0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0]': 1, '[1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0]': 1, '[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]': 1, '[0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0]': 1, '[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0]': 1, '[1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0]': 1, '[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]': 1, '[1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]': 1, '[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]': 1, '[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0]': 1, '[1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0]': 1}\n",
      "2025-12-26 01:19:59,352 - INFO - Final dataset shape: (3088, 46)\n",
      "2025-12-26 01:19:59,352 - INFO - Final columns: ['abbreviation_density', 'average_cyclomatic_complexity', 'avg_line_length', 'comment_code_mismatch_score', 'comment_lines', 'comment_percentage', 'decision_density', 'documentation_coverage', 'external_vs_internal_field_access_ratio', 'functions', 'global_usages_total', 'globals_declared', 'halstead_difficulty', 'halstead_effort', 'halstead_estimated_bugs', 'halstead_volume', 'inter_file_coupling', 'large_parameter_list_indicator', 'lazy_class_indicator', 'lines_added', 'lines_of_code', 'long_method_indicator', 'maintainability_score', 'max_cyclomatic_ratio', 'max_line_length', 'max_lines_per_function', 'max_nesting_level', 'mean_cyclomatic_ratio', 'mean_lines_per_function', 'mean_param_entropy', 'nesting_variance', 'percent_lines_over_80', 'source_lines', 'test_files_found', 'test_function_count', 'test_lines', 'test_to_source_ratio', 'total_imports', 'y_binary', 'complexity_score', 'code_health', 'doc_quality', 'has_tests', 'coupling_complexity', 'smell_density', 'effort_impact_ratio']\n",
      "2025-12-26 01:19:59,353 - INFO - \n",
      "============================================================\n",
      "2025-12-26 01:19:59,353 - INFO - DATASET SUMMARY STATISTICS\n",
      "2025-12-26 01:19:59,353 - INFO - ============================================================\n",
      "2025-12-26 01:19:59,353 - INFO - \n",
      "Original shape: (3088, 67)\n",
      "2025-12-26 01:19:59,354 - INFO - Final shape: (3088, 46)\n",
      "2025-12-26 01:19:59,357 - INFO - \n",
      "Numeric columns: 42\n",
      "2025-12-26 01:19:59,358 - INFO - Categorical columns: 1\n",
      "2025-12-26 01:19:59,359 - INFO - ============================================================\n",
      "\n",
      "2025-12-26 01:19:59,359 - INFO - Pipeline complete! Saving to ../../data/processed/dataset_processed.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize the feature builder\n",
    "builder = CodeQualityFeatureBuilder(input_file, output_file)\n",
    "\n",
    "# Build features\n",
    "processed_df = builder.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 01:19:59,457 - INFO - Data successfully saved to ../../data/processed/dataset_processed.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "✓ Feature engineering complete!\n",
      "✓ Output saved to: ../../data/processed/dataset_processed.csv\n",
      "✓ Ready for machine learning models\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "builder.save()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ Feature engineering complete!\")\n",
    "print(f\"✓ Output saved to: {output_file}\")\n",
    "print(f\"✓ Ready for machine learning models\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Summary\n",
    "\n",
    "### 8 Engineered Features Created:\n",
    "\n",
    "1. **complexity_score** - Aggregated code complexity (mean of cyclomatic complexity metrics)\n",
    "   - Combines: average_cyclomatic_complexity, max_cyclomatic_ratio, mean_cyclomatic_ratio\n",
    "\n",
    "2. **code_health** - Overall health indicator\n",
    "   - Weighted combination: 30% PEP8 compliance + 40% maintainability + 30% comment accuracy\n",
    "\n",
    "3. **doc_quality** - Documentation quality score\n",
    "   - Balanced: 50% documentation coverage + 50% comment percentage\n",
    "\n",
    "4. **has_tests** - Test presence flag\n",
    "   - Binary indicator: 1 if test_to_source_ratio > 0, else 0\n",
    "\n",
    "5. **coupling_complexity** - Inter-file coupling metric\n",
    "   - Weighted average: 50% inter_file_coupling + 50% call_graph_density\n",
    "\n",
    "6. **smell_density** - Code smell frequency\n",
    "   - Normalized count: number of smells per 100 lines of code\n",
    "\n",
    "7. **effort_impact_ratio** - Development effort vs impact\n",
    "   - Ratio: halstead_effort / (halstead_estimated_bugs + 1)\n",
    "\n",
    "8. **file_maturity** - Combined age and change metrics\n",
    "   - Log-age × change frequency: log(file_age_days + 1) × (1 + lines_added / LOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbreviation_density</th>\n",
       "      <th>average_cyclomatic_complexity</th>\n",
       "      <th>avg_line_length</th>\n",
       "      <th>comment_code_mismatch_score</th>\n",
       "      <th>comment_lines</th>\n",
       "      <th>comment_percentage</th>\n",
       "      <th>decision_density</th>\n",
       "      <th>documentation_coverage</th>\n",
       "      <th>external_vs_internal_field_access_ratio</th>\n",
       "      <th>functions</th>\n",
       "      <th>...</th>\n",
       "      <th>test_to_source_ratio</th>\n",
       "      <th>total_imports</th>\n",
       "      <th>y_binary</th>\n",
       "      <th>complexity_score</th>\n",
       "      <th>code_health</th>\n",
       "      <th>doc_quality</th>\n",
       "      <th>has_tests</th>\n",
       "      <th>coupling_complexity</th>\n",
       "      <th>smell_density</th>\n",
       "      <th>effort_impact_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.666324</td>\n",
       "      <td>0.568106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.623314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0]</td>\n",
       "      <td>0.718428</td>\n",
       "      <td>0.880992</td>\n",
       "      <td>0.4165</td>\n",
       "      <td>0</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.150928</td>\n",
       "      <td>0.449420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.306982</td>\n",
       "      <td>0.531561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.326629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>0.352370</td>\n",
       "      <td>0.932637</td>\n",
       "      <td>0.4525</td>\n",
       "      <td>1</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.402474</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333</td>\n",
       "      <td>0.204312</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>0.300120</td>\n",
       "      <td>0.929269</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.551215</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.204312</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>0.279220</td>\n",
       "      <td>0.932637</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.291447</td>\n",
       "      <td>0.019656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.204312</td>\n",
       "      <td>0.320598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>0.279220</td>\n",
       "      <td>0.932637</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.272644</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abbreviation_density  average_cyclomatic_complexity  avg_line_length  \\\n",
       "0                 0.000                       0.666324         0.568106   \n",
       "1                 0.000                       0.306982         0.531561   \n",
       "2                 0.333                       0.204312         0.348837   \n",
       "3                 0.500                       0.204312         0.325581   \n",
       "4                 0.500                       0.204312         0.320598   \n",
       "\n",
       "   comment_code_mismatch_score  comment_lines  comment_percentage  \\\n",
       "0                          0.0           1.00                 1.0   \n",
       "1                          0.0           0.25                 1.0   \n",
       "2                          0.0           0.00                 0.0   \n",
       "3                          0.0           0.00                 0.0   \n",
       "4                          0.0           0.00                 0.0   \n",
       "\n",
       "   decision_density  documentation_coverage  \\\n",
       "0          0.623314                     0.0   \n",
       "1          0.326629                     0.0   \n",
       "2          0.163200                     0.0   \n",
       "3          0.104000                     0.0   \n",
       "4          0.099429                     0.0   \n",
       "\n",
       "   external_vs_internal_field_access_ratio  functions  ...  \\\n",
       "0                                 0.458333     0.1250  ...   \n",
       "1                                 0.125000     0.0625  ...   \n",
       "2                                 0.041667     0.0000  ...   \n",
       "3                                 0.041667     0.0000  ...   \n",
       "4                                 0.041667     0.0000  ...   \n",
       "\n",
       "   test_to_source_ratio  total_imports  \\\n",
       "0                   0.0       0.000000   \n",
       "1                   1.0       0.230769   \n",
       "2                   0.0       0.230769   \n",
       "3                   0.0       0.230769   \n",
       "4                   0.0       0.307692   \n",
       "\n",
       "                                  y_binary  complexity_score  code_health  \\\n",
       "0  [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0]          0.718428     0.880992   \n",
       "1  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]          0.352370     0.932637   \n",
       "2  [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]          0.300120     0.929269   \n",
       "3  [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]          0.279220     0.932637   \n",
       "4  [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]          0.279220     0.932637   \n",
       "\n",
       "   doc_quality  has_tests  coupling_complexity  smell_density  \\\n",
       "0       0.4165          0             0.541667       0.150928   \n",
       "1       0.4525          1             0.083333       0.402474   \n",
       "2       0.5000          0             0.083333       0.551215   \n",
       "3       0.5000          0             0.083333       0.291447   \n",
       "4       0.5000          0             0.083333       0.272644   \n",
       "\n",
       "   effort_impact_ratio  \n",
       "0             0.449420  \n",
       "1             0.000000  \n",
       "2             0.000000  \n",
       "3             0.019656  \n",
       "4             0.000000  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset shape: (3088, 46)\n",
      "\n",
      "Column names:\n",
      "['abbreviation_density', 'average_cyclomatic_complexity', 'avg_line_length', 'comment_code_mismatch_score', 'comment_lines', 'comment_percentage', 'decision_density', 'documentation_coverage', 'external_vs_internal_field_access_ratio', 'functions', 'global_usages_total', 'globals_declared', 'halstead_difficulty', 'halstead_effort', 'halstead_estimated_bugs', 'halstead_volume', 'inter_file_coupling', 'large_parameter_list_indicator', 'lazy_class_indicator', 'lines_added', 'lines_of_code', 'long_method_indicator', 'maintainability_score', 'max_cyclomatic_ratio', 'max_line_length', 'max_lines_per_function', 'max_nesting_level', 'mean_cyclomatic_ratio', 'mean_lines_per_function', 'mean_param_entropy', 'nesting_variance', 'percent_lines_over_80', 'source_lines', 'test_files_found', 'test_function_count', 'test_lines', 'test_to_source_ratio', 'total_imports', 'y_binary', 'complexity_score', 'code_health', 'doc_quality', 'has_tests', 'coupling_complexity', 'smell_density', 'effort_impact_ratio']\n"
     ]
    }
   ],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows:\")\n",
    "display(processed_df.head())\n",
    "\n",
    "print(\"\\nDataset shape:\", processed_df.shape)\n",
    "print(\"\\nColumn names:\")\n",
    "print(processed_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "abbreviation_density                       float64\n",
       "average_cyclomatic_complexity              float64\n",
       "avg_line_length                            float64\n",
       "comment_code_mismatch_score                float64\n",
       "comment_lines                              float64\n",
       "comment_percentage                         float64\n",
       "decision_density                           float64\n",
       "documentation_coverage                     float64\n",
       "external_vs_internal_field_access_ratio    float64\n",
       "functions                                  float64\n",
       "global_usages_total                        float64\n",
       "globals_declared                           float64\n",
       "halstead_difficulty                        float64\n",
       "halstead_effort                            float64\n",
       "halstead_estimated_bugs                    float64\n",
       "halstead_volume                            float64\n",
       "inter_file_coupling                        float64\n",
       "large_parameter_list_indicator                bool\n",
       "lazy_class_indicator                          bool\n",
       "lines_added                                float64\n",
       "lines_of_code                              float64\n",
       "long_method_indicator                         bool\n",
       "maintainability_score                      float64\n",
       "max_cyclomatic_ratio                       float64\n",
       "max_line_length                            float64\n",
       "max_lines_per_function                     float64\n",
       "max_nesting_level                          float64\n",
       "mean_cyclomatic_ratio                      float64\n",
       "mean_lines_per_function                    float64\n",
       "mean_param_entropy                         float64\n",
       "nesting_variance                           float64\n",
       "percent_lines_over_80                      float64\n",
       "source_lines                               float64\n",
       "test_files_found                           float64\n",
       "test_function_count                        float64\n",
       "test_lines                                 float64\n",
       "test_to_source_ratio                       float64\n",
       "total_imports                              float64\n",
       "y_binary                                    object\n",
       "complexity_score                           float64\n",
       "code_health                                float64\n",
       "doc_quality                                float64\n",
       "has_tests                                    int64\n",
       "coupling_complexity                        float64\n",
       "smell_density                              float64\n",
       "effort_impact_ratio                        float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbreviation_density</th>\n",
       "      <th>average_cyclomatic_complexity</th>\n",
       "      <th>avg_line_length</th>\n",
       "      <th>comment_code_mismatch_score</th>\n",
       "      <th>comment_lines</th>\n",
       "      <th>comment_percentage</th>\n",
       "      <th>decision_density</th>\n",
       "      <th>documentation_coverage</th>\n",
       "      <th>external_vs_internal_field_access_ratio</th>\n",
       "      <th>functions</th>\n",
       "      <th>...</th>\n",
       "      <th>test_lines</th>\n",
       "      <th>test_to_source_ratio</th>\n",
       "      <th>total_imports</th>\n",
       "      <th>complexity_score</th>\n",
       "      <th>code_health</th>\n",
       "      <th>doc_quality</th>\n",
       "      <th>has_tests</th>\n",
       "      <th>coupling_complexity</th>\n",
       "      <th>smell_density</th>\n",
       "      <th>effort_impact_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.154492</td>\n",
       "      <td>0.206129</td>\n",
       "      <td>0.402521</td>\n",
       "      <td>0.339418</td>\n",
       "      <td>0.198267</td>\n",
       "      <td>0.196313</td>\n",
       "      <td>0.209951</td>\n",
       "      <td>0.331658</td>\n",
       "      <td>0.205455</td>\n",
       "      <td>0.184545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203447</td>\n",
       "      <td>0.226410</td>\n",
       "      <td>0.235627</td>\n",
       "      <td>0.253421</td>\n",
       "      <td>0.813694</td>\n",
       "      <td>0.653175</td>\n",
       "      <td>0.349741</td>\n",
       "      <td>0.168510</td>\n",
       "      <td>0.246824</td>\n",
       "      <td>0.198678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.225290</td>\n",
       "      <td>0.202331</td>\n",
       "      <td>0.172328</td>\n",
       "      <td>0.421430</td>\n",
       "      <td>0.339540</td>\n",
       "      <td>0.340474</td>\n",
       "      <td>0.242047</td>\n",
       "      <td>0.441740</td>\n",
       "      <td>0.296730</td>\n",
       "      <td>0.240025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366866</td>\n",
       "      <td>0.387395</td>\n",
       "      <td>0.226940</td>\n",
       "      <td>0.199899</td>\n",
       "      <td>0.142631</td>\n",
       "      <td>0.221390</td>\n",
       "      <td>0.476965</td>\n",
       "      <td>0.217356</td>\n",
       "      <td>0.201782</td>\n",
       "      <td>0.328418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075975</td>\n",
       "      <td>0.342193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.100057</td>\n",
       "      <td>0.699478</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112095</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.101643</td>\n",
       "      <td>0.431894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.195622</td>\n",
       "      <td>0.898956</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.211299</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.306982</td>\n",
       "      <td>0.506645</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.381029</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.325043</td>\n",
       "      <td>0.932637</td>\n",
       "      <td>0.938125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.334071</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       abbreviation_density  average_cyclomatic_complexity  avg_line_length  \\\n",
       "count           3088.000000                    3088.000000      3088.000000   \n",
       "mean               0.154492                       0.206129         0.402521   \n",
       "std                0.225290                       0.202331         0.172328   \n",
       "min                0.000000                       0.000000         0.000000   \n",
       "25%                0.000000                       0.075975         0.342193   \n",
       "50%                0.053000                       0.101643         0.431894   \n",
       "75%                0.250000                       0.306982         0.506645   \n",
       "max                1.000000                       1.000000         1.000000   \n",
       "\n",
       "       comment_code_mismatch_score  comment_lines  comment_percentage  \\\n",
       "count                  3088.000000    3088.000000         3088.000000   \n",
       "mean                      0.339418       0.198267            0.196313   \n",
       "std                       0.421430       0.339540            0.340474   \n",
       "min                       0.000000       0.000000            0.000000   \n",
       "25%                       0.000000       0.000000            0.000000   \n",
       "50%                       0.000000       0.000000            0.000000   \n",
       "75%                       0.846000       0.250000            0.250000   \n",
       "max                       1.000000       1.000000            1.000000   \n",
       "\n",
       "       decision_density  documentation_coverage  \\\n",
       "count       3088.000000             3088.000000   \n",
       "mean           0.209951                0.331658   \n",
       "std            0.242047                0.441740   \n",
       "min            0.000000                0.000000   \n",
       "25%            0.000000                0.000000   \n",
       "50%            0.120800                0.000000   \n",
       "75%            0.381029                1.000000   \n",
       "max            1.000000                1.000000   \n",
       "\n",
       "       external_vs_internal_field_access_ratio    functions  ...   test_lines  \\\n",
       "count                              3088.000000  3088.000000  ...  3088.000000   \n",
       "mean                                  0.205455     0.184545  ...     0.203447   \n",
       "std                                   0.296730     0.240025  ...     0.366866   \n",
       "min                                   0.000000     0.000000  ...     0.000000   \n",
       "25%                                   0.000000     0.000000  ...     0.000000   \n",
       "50%                                   0.083333     0.125000  ...     0.000000   \n",
       "75%                                   0.250000     0.250000  ...     0.250000   \n",
       "max                                   1.000000     1.000000  ...     1.000000   \n",
       "\n",
       "       test_to_source_ratio  total_imports  complexity_score  code_health  \\\n",
       "count           3088.000000    3088.000000       3088.000000  3088.000000   \n",
       "mean               0.226410       0.235627          0.253421     0.813694   \n",
       "std                0.387395       0.226940          0.199899     0.142631   \n",
       "min                0.000000       0.000000          0.000000     0.000000   \n",
       "25%                0.000000       0.076923          0.100057     0.699478   \n",
       "50%                0.000000       0.153846          0.195622     0.898956   \n",
       "75%                0.250000       0.307692          0.325043     0.932637   \n",
       "max                1.000000       1.000000          1.000000     1.000000   \n",
       "\n",
       "       doc_quality    has_tests  coupling_complexity  smell_density  \\\n",
       "count  3088.000000  3088.000000          3088.000000    3088.000000   \n",
       "mean      0.653175     0.349741             0.168510       0.246824   \n",
       "std       0.221390     0.476965             0.217356       0.201782   \n",
       "min       0.000000     0.000000             0.000000       0.000000   \n",
       "25%       0.500000     0.000000             0.000000       0.112095   \n",
       "50%       0.500000     0.000000             0.083333       0.211299   \n",
       "75%       0.938125     1.000000             0.250000       0.334071   \n",
       "max       1.000000     1.000000             1.000000       1.000000   \n",
       "\n",
       "       effort_impact_ratio  \n",
       "count          3088.000000  \n",
       "mean              0.198678  \n",
       "std               0.328418  \n",
       "min               0.000000  \n",
       "25%               0.000000  \n",
       "50%               0.000000  \n",
       "75%               0.250000  \n",
       "max               1.000000  \n",
       "\n",
       "[8 rows x 42 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data types\n",
    "print(\"Data types:\")\n",
    "display(processed_df.dtypes)\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "display(processed_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Total missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "missing = processed_df.isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "print(f\"\\nTotal missing values: {processed_df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable distribution:\n",
      "y_binary\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    403\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]    175\n",
      "[0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0]    149\n",
      "[0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]     97\n",
      "[0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]     89\n",
      "                                          ... \n",
      "[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]      1\n",
      "[1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]      1\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]      1\n",
      "[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0]      1\n",
      "[1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0]      1\n",
      "Name: count, Length: 294, dtype: int64\n",
      "\n",
      "Target balance:\n",
      "y_binary\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.130505\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]    0.056671\n",
      "[0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0]    0.048251\n",
      "[0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]    0.031412\n",
      "[0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]    0.028821\n",
      "                                             ...   \n",
      "[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]    0.000324\n",
      "[1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]    0.000324\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]    0.000324\n",
      "[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0]    0.000324\n",
      "[1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0]    0.000324\n",
      "Name: proportion, Length: 294, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Target variable distribution\n",
    "if 'y_binary' in processed_df.columns:\n",
    "    print(\"Target variable distribution:\")\n",
    "    print(processed_df['y_binary'].value_counts())\n",
    "    print(f\"\\nTarget balance:\")\n",
    "    print(processed_df['y_binary'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered Features Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complexity_score</th>\n",
       "      <th>code_health</th>\n",
       "      <th>doc_quality</th>\n",
       "      <th>has_tests</th>\n",
       "      <th>coupling_complexity</th>\n",
       "      <th>smell_density</th>\n",
       "      <th>effort_impact_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.253421</td>\n",
       "      <td>0.813694</td>\n",
       "      <td>0.653175</td>\n",
       "      <td>0.349741</td>\n",
       "      <td>0.168510</td>\n",
       "      <td>0.246824</td>\n",
       "      <td>0.198678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.199899</td>\n",
       "      <td>0.142631</td>\n",
       "      <td>0.221390</td>\n",
       "      <td>0.476965</td>\n",
       "      <td>0.217356</td>\n",
       "      <td>0.201782</td>\n",
       "      <td>0.328418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.100057</td>\n",
       "      <td>0.699478</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112095</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.195622</td>\n",
       "      <td>0.898956</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.211299</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.325043</td>\n",
       "      <td>0.932637</td>\n",
       "      <td>0.938125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.334071</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       complexity_score  code_health  doc_quality    has_tests  \\\n",
       "count       3088.000000  3088.000000  3088.000000  3088.000000   \n",
       "mean           0.253421     0.813694     0.653175     0.349741   \n",
       "std            0.199899     0.142631     0.221390     0.476965   \n",
       "min            0.000000     0.000000     0.000000     0.000000   \n",
       "25%            0.100057     0.699478     0.500000     0.000000   \n",
       "50%            0.195622     0.898956     0.500000     0.000000   \n",
       "75%            0.325043     0.932637     0.938125     1.000000   \n",
       "max            1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       coupling_complexity  smell_density  effort_impact_ratio  \n",
       "count          3088.000000    3088.000000          3088.000000  \n",
       "mean              0.168510       0.246824             0.198678  \n",
       "std               0.217356       0.201782             0.328418  \n",
       "min               0.000000       0.000000             0.000000  \n",
       "25%               0.000000       0.112095             0.000000  \n",
       "50%               0.083333       0.211299             0.000000  \n",
       "75%               0.250000       0.334071             0.250000  \n",
       "max               1.000000       1.000000             1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Engineered Features Correlation Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complexity_score</th>\n",
       "      <th>code_health</th>\n",
       "      <th>doc_quality</th>\n",
       "      <th>has_tests</th>\n",
       "      <th>coupling_complexity</th>\n",
       "      <th>smell_density</th>\n",
       "      <th>effort_impact_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>complexity_score</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.410814</td>\n",
       "      <td>0.553501</td>\n",
       "      <td>-0.233257</td>\n",
       "      <td>0.053731</td>\n",
       "      <td>0.050480</td>\n",
       "      <td>0.253557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code_health</th>\n",
       "      <td>-0.410814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.614805</td>\n",
       "      <td>0.120926</td>\n",
       "      <td>-0.339119</td>\n",
       "      <td>0.220539</td>\n",
       "      <td>-0.607025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_quality</th>\n",
       "      <td>0.553501</td>\n",
       "      <td>-0.614805</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.205074</td>\n",
       "      <td>-0.155969</td>\n",
       "      <td>-0.083146</td>\n",
       "      <td>0.310195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_tests</th>\n",
       "      <td>-0.233257</td>\n",
       "      <td>0.120926</td>\n",
       "      <td>-0.205074</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093369</td>\n",
       "      <td>-0.157772</td>\n",
       "      <td>-0.048950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coupling_complexity</th>\n",
       "      <td>0.053731</td>\n",
       "      <td>-0.339119</td>\n",
       "      <td>-0.155969</td>\n",
       "      <td>0.093369</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.094573</td>\n",
       "      <td>0.409782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smell_density</th>\n",
       "      <td>0.050480</td>\n",
       "      <td>0.220539</td>\n",
       "      <td>-0.083146</td>\n",
       "      <td>-0.157772</td>\n",
       "      <td>-0.094573</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.290651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>effort_impact_ratio</th>\n",
       "      <td>0.253557</td>\n",
       "      <td>-0.607025</td>\n",
       "      <td>0.310195</td>\n",
       "      <td>-0.048950</td>\n",
       "      <td>0.409782</td>\n",
       "      <td>-0.290651</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     complexity_score  code_health  doc_quality  has_tests  \\\n",
       "complexity_score             1.000000    -0.410814     0.553501  -0.233257   \n",
       "code_health                 -0.410814     1.000000    -0.614805   0.120926   \n",
       "doc_quality                  0.553501    -0.614805     1.000000  -0.205074   \n",
       "has_tests                   -0.233257     0.120926    -0.205074   1.000000   \n",
       "coupling_complexity          0.053731    -0.339119    -0.155969   0.093369   \n",
       "smell_density                0.050480     0.220539    -0.083146  -0.157772   \n",
       "effort_impact_ratio          0.253557    -0.607025     0.310195  -0.048950   \n",
       "\n",
       "                     coupling_complexity  smell_density  effort_impact_ratio  \n",
       "complexity_score                0.053731       0.050480             0.253557  \n",
       "code_health                    -0.339119       0.220539            -0.607025  \n",
       "doc_quality                    -0.155969      -0.083146             0.310195  \n",
       "has_tests                       0.093369      -0.157772            -0.048950  \n",
       "coupling_complexity             1.000000      -0.094573             0.409782  \n",
       "smell_density                  -0.094573       1.000000            -0.290651  \n",
       "effort_impact_ratio             0.409782      -0.290651             1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display engineered features statistics\n",
    "engineered_features = ['complexity_score', 'code_health', 'doc_quality', 'has_tests', \n",
    "                       'coupling_complexity', 'smell_density', 'effort_impact_ratio', 'file_maturity']\n",
    "\n",
    "existing_features = [f for f in engineered_features if f in processed_df.columns]\n",
    "\n",
    "if existing_features:\n",
    "    print(\"Engineered Features Statistics:\")\n",
    "    display(processed_df[existing_features].describe())\n",
    "    \n",
    "    print(\"\\n\\nEngineered Features Correlation Matrix:\")\n",
    "    display(processed_df[existing_features].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineered Features Insights:\n",
      "======================================================================\n",
      "✓ complexity_score: min=0.0000, max=1.0000, mean=0.2534\n",
      "✓ code_health: min=0.0000, max=1.0000, mean=0.8137\n",
      "✓ doc_quality: min=0.0000, max=1.0000, mean=0.6532\n",
      "✓ has_tests distribution: {0: 2008, 1: 1080}\n",
      "✓ coupling_complexity: min=0.0000, max=1.0000, mean=0.1685\n",
      "✓ smell_density: min=0.0000, max=1.0000, mean=0.2468\n",
      "✓ effort_impact_ratio: min=0.0000, max=1.0000, mean=0.1987\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Feature importance insights\n",
    "print(\"\\nEngineered Features Insights:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if 'complexity_score' in processed_df.columns:\n",
    "    print(f\"✓ complexity_score: min={processed_df['complexity_score'].min():.4f}, \"\n",
    "          f\"max={processed_df['complexity_score'].max():.4f}, \"\n",
    "          f\"mean={processed_df['complexity_score'].mean():.4f}\")\n",
    "\n",
    "if 'code_health' in processed_df.columns:\n",
    "    print(f\"✓ code_health: min={processed_df['code_health'].min():.4f}, \"\n",
    "          f\"max={processed_df['code_health'].max():.4f}, \"\n",
    "          f\"mean={processed_df['code_health'].mean():.4f}\")\n",
    "\n",
    "if 'doc_quality' in processed_df.columns:\n",
    "    print(f\"✓ doc_quality: min={processed_df['doc_quality'].min():.4f}, \"\n",
    "          f\"max={processed_df['doc_quality'].max():.4f}, \"\n",
    "          f\"mean={processed_df['doc_quality'].mean():.4f}\")\n",
    "\n",
    "if 'has_tests' in processed_df.columns:\n",
    "    print(f\"✓ has_tests distribution: {processed_df['has_tests'].value_counts().to_dict()}\")\n",
    "\n",
    "if 'coupling_complexity' in processed_df.columns:\n",
    "    print(f\"✓ coupling_complexity: min={processed_df['coupling_complexity'].min():.4f}, \"\n",
    "          f\"max={processed_df['coupling_complexity'].max():.4f}, \"\n",
    "          f\"mean={processed_df['coupling_complexity'].mean():.4f}\")\n",
    "\n",
    "if 'smell_density' in processed_df.columns:\n",
    "    print(f\"✓ smell_density: min={processed_df['smell_density'].min():.4f}, \"\n",
    "          f\"max={processed_df['smell_density'].max():.4f}, \"\n",
    "          f\"mean={processed_df['smell_density'].mean():.4f}\")\n",
    "\n",
    "if 'effort_impact_ratio' in processed_df.columns:\n",
    "    print(f\"✓ effort_impact_ratio: min={processed_df['effort_impact_ratio'].min():.4f}, \"\n",
    "          f\"max={processed_df['effort_impact_ratio'].max():.4f}, \"\n",
    "          f\"mean={processed_df['effort_impact_ratio'].mean():.4f}\")\n",
    "\n",
    "if 'file_maturity' in processed_df.columns:\n",
    "    print(f\"✓ file_maturity: min={processed_df['file_maturity'].min():.4f}, \"\n",
    "          f\"max={processed_df['file_maturity'].max():.4f}, \"\n",
    "          f\"mean={processed_df['file_maturity'].mean():.4f}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
