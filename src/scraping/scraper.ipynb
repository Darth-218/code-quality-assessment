{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f5d5b3a",
   "metadata": {},
   "source": [
    "# GitHub Repository Scraper\n",
    "## Query GitHub REST API to find repositories and store metadata\n",
    "\n",
    "This notebook searches for GitHub repositories using various criteria and saves their metadata for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da767bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c728cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "GITHUB_TOKEN = \"Removed_Token_For_Security\"\n",
    "MAX_REPOS = 1000\n",
    "OUTPUT_FILE = \"repository_list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7ac694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_queries_from_user():\n",
    "    print(\"Choose an option:\")\n",
    "    print(\"1. Enter custom search queries\")\n",
    "    print(\"2. Use default queries\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice = input(\"\\nEnter your choice (1/2): \").strip()\n",
    "            if choice == \"1\":\n",
    "                print(\"Enter your search queries (one per line).\")\n",
    "                print(\"Examples:\")\n",
    "                print(\"  language:python stars:>100\")\n",
    "                print(\"  topic:machine-learning\")\n",
    "                print(\"  web framework in:description\")\n",
    "                print(\"Press Enter twice when finished.\\n\")\n",
    "                \n",
    "                queries = []\n",
    "                while True:\n",
    "                    query = input(\"Enter query: \").strip()\n",
    "                    if query == \"\":\n",
    "                        if queries:\n",
    "                            break\n",
    "                        else:\n",
    "                            print(\"No queries entered. Using default queries.\")\n",
    "                            return 0\n",
    "                    queries.append(query)\n",
    "                return queries\n",
    "                \n",
    "            elif choice == \"2\":\n",
    "                return 0  # Signal to use defaults\n",
    "            else:\n",
    "                print(\"Invalid choice. Please enter 1 or 2.\")\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nOperation cancelled.\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b420ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup session\n",
    "session = requests.Session()\n",
    "headers = {\n",
    "    'Accept': 'application/vnd.github.v3+json',\n",
    "    'User-Agent': 'GitHub-Repo-Scraper/1.0'\n",
    "}\n",
    "\n",
    "if GITHUB_TOKEN:\n",
    "    headers['Authorization'] = f'token {GITHUB_TOKEN}'\n",
    "session.headers.update(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42129b",
   "metadata": {},
   "source": [
    "- Creates a requests.Session() to maintain connection and headers\n",
    "- Sets API headers that tell GitHub:\n",
    "    - We want version 3 of their API\n",
    "    - Our application name for identification\n",
    "    - Authorization token if provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9221c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GitHub API rate limits\n",
    "def check_rate_limit():\n",
    "    try:\n",
    "        response = session.get(\"https://api.github.com/rate_limit\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            core = data['resources']['core']\n",
    "            rate_limit_remaining = core['remaining']\n",
    "            \n",
    "            if rate_limit_remaining < 10:\n",
    "                reset_time = datetime.fromtimestamp(core['reset'])\n",
    "                wait_time = (reset_time - datetime.now()).total_seconds()\n",
    "                if wait_time > 0:\n",
    "                    print(f\"Rate limit low. Waiting {wait_time/60:.1f} minutes\")\n",
    "                    time.sleep(wait_time + 10)\n",
    "                return False\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking rate limit: {e}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b46aac8",
   "metadata": {},
   "source": [
    "Purpose: Prevents hitting GitHub's API limits\n",
    "How it works:\n",
    "\n",
    "- Calls GitHub's rate limit endpoint\n",
    "- Checks how many requests remain\n",
    "- If low (<10), calculates when limit resets\n",
    "- Waits automatically if needed\n",
    "- Returns False if we should pause, True if we can continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8921377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant metadata from repository data\n",
    "def extract_repo_metadata(repo_data):\n",
    "    return {\n",
    "        'id': repo_data['id'],\n",
    "        'name': repo_data['name'],\n",
    "        'full_name': repo_data['full_name'],\n",
    "        'html_url': repo_data['html_url'],\n",
    "        'clone_url': repo_data['clone_url'],\n",
    "        'description': repo_data.get('description', ''),\n",
    "        'language': repo_data.get('language'),\n",
    "        'created_at': repo_data['created_at'],\n",
    "        'updated_at': repo_data['updated_at'],\n",
    "        'size': repo_data['size'],\n",
    "        'stargazers_count': repo_data['stargazers_count'],\n",
    "        'forks_count': repo_data['forks_count'],\n",
    "        'open_issues_count': repo_data['open_issues_count'],\n",
    "        'license': repo_data.get('license', {}).get('key') if repo_data.get('license') else None,\n",
    "        'topics': repo_data.get('topics', []),\n",
    "        'owner_login': repo_data['owner']['login'],\n",
    "        'is_fork': repo_data['fork'],\n",
    "        'is_archived': repo_data.get('archived', False),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3888e4",
   "metadata": {},
   "source": [
    "Purpose: Cleans and organizes raw repository data\n",
    "Extracts 16 key fields:\n",
    "\n",
    "- Basic info: id, name, full_name, description\n",
    "- URLs: html_url (browser link), clone_url (git clone)\n",
    "- Technical: language, size (KB), topics (tags)\n",
    "- Statistics: stargazers_count, forks_count, open_issues_count\n",
    "- Dates: created_at, updated_at\n",
    "- Status: is_fork, is_archived\n",
    "- Ownership: owner_login, license"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26efaf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search repositories with given query\n",
    "def search_repositories(query, max_repos=100):\n",
    "    if not check_rate_limit():\n",
    "        return []\n",
    "        \n",
    "    repositories = []\n",
    "    page = 1\n",
    "    \n",
    "    while len(repositories) < max_repos:\n",
    "        url = \"https://api.github.com/search/repositories\"\n",
    "        params = {\n",
    "            'q': query,\n",
    "            'sort': 'stars',\n",
    "            'order': 'desc',\n",
    "            'per_page': 100,\n",
    "            'page': page\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            print(f\"Fetching page {page} for: {query}\")\n",
    "            response = session.get(url, params=params)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                items = data.get('items', [])\n",
    "                \n",
    "                if not items:\n",
    "                    break\n",
    "                \n",
    "                for repo in items:\n",
    "                    repo_data = extract_repo_metadata(repo)\n",
    "                    repositories.append(repo_data)\n",
    "                \n",
    "                print(f\"Page {page}: {len(items)} repos (total: {len(repositories)})\")\n",
    "                \n",
    "                if len(repositories) >= data.get('total_count', 0):\n",
    "                    break\n",
    "                    \n",
    "                page += 1\n",
    "                time.sleep(1)\n",
    "                \n",
    "            elif response.status_code == 403:\n",
    "                print(\"Rate limit exceeded. Waiting 60 seconds...\")\n",
    "                time.sleep(60)\n",
    "            else:\n",
    "                print(f\"API error: {response.status_code}\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "            \n",
    "    return repositories[:max_repos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4a0e7",
   "metadata": {},
   "source": [
    "Purpose: Main function that fetches repositories from GitHub\n",
    "Step-by-step process:\n",
    "\n",
    "- Checks rate limits first\n",
    "- Uses pagination to get multiple pages (100 repos/page)\n",
    "- Sends search query to GitHub API\n",
    "- Processes each repository through extract_repo_metadata()\n",
    "- Handles errors and rate limits gracefully\n",
    "- Returns clean repository list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bc75b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save repository list\n",
    "def save_repo_list(repositories, output_file):\n",
    "    os.makedirs('../../data/repo_details', exist_ok=True)\n",
    "    \n",
    "    # Save as JSON\n",
    "    with open(f'../../data/repo_details/{output_file}.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(repositories, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Save as CSV for easy viewing\n",
    "    df = pd.DataFrame(repositories)\n",
    "    df.to_csv(f'../../data/repo_details/{output_file}.csv', index=False)\n",
    "    \n",
    "    # Save simple list for downloader\n",
    "    download_list = [{\n",
    "        'full_name': repo['full_name'],\n",
    "        'clone_url': repo['clone_url'],\n",
    "        'language': repo['language']\n",
    "    } for repo in repositories]\n",
    "    \n",
    "    with open(f'../../data/repo_details/{output_file}_download_list.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(download_list, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Saved {len(repositories)} repositories to data/{output_file}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df07088",
   "metadata": {},
   "source": [
    "Purpose: Saves collected data in multiple formats\n",
    "Creates 3 files:\n",
    "\n",
    "- JSON: Full dataset for programs to read\n",
    "- CSV: Spreadsheet format to view\n",
    "- Simplified JSON: Only essential fields for downloading repos later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04d4f249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose an option:\n",
      "1. Enter custom search queries\n",
      "2. Use default queries\n",
      "Using default queries.\n",
      "\n",
      "Search 1/3: language:python stars:>100\n",
      "Fetching page 1 for: language:python stars:>100\n",
      "Page 1: 100 repos (total: 100)\n",
      "Fetching page 2 for: language:python stars:>100\n",
      "Page 2: 100 repos (total: 200)\n",
      "Fetching page 3 for: language:python stars:>100\n",
      "Page 3: 100 repos (total: 300)\n",
      "Fetching page 4 for: language:python stars:>100\n",
      "Page 4: 100 repos (total: 400)\n",
      "Total unique repositories: 333\n",
      "\n",
      "Search 2/3: language:java stars:>100\n",
      "Fetching page 1 for: language:java stars:>100\n",
      "Page 1: 100 repos (total: 100)\n",
      "Fetching page 2 for: language:java stars:>100\n",
      "Page 2: 100 repos (total: 200)\n",
      "Fetching page 3 for: language:java stars:>100\n",
      "Page 3: 100 repos (total: 300)\n",
      "Fetching page 4 for: language:java stars:>100\n",
      "Page 4: 100 repos (total: 400)\n",
      "Total unique repositories: 666\n",
      "\n",
      "Search 3/3: language:cpp stars:>100\n",
      "Fetching page 1 for: language:cpp stars:>100\n",
      "Page 1: 100 repos (total: 100)\n",
      "Fetching page 2 for: language:cpp stars:>100\n",
      "Page 2: 100 repos (total: 200)\n",
      "Fetching page 3 for: language:cpp stars:>100\n",
      "Page 3: 100 repos (total: 300)\n",
      "Fetching page 4 for: language:cpp stars:>100\n",
      "Page 4: 100 repos (total: 400)\n",
      "Total unique repositories: 999\n",
      "Saved 999 repositories to data/repository_list.json\n",
      "\n",
      "Scraping complete! 999 repositories saved.\n",
      "\n",
      "Summary:\n",
      "Languages found: 3\n",
      "Total stars: 27,062,600\n",
      "Total forks: 5,061,266\n",
      "\n",
      "Language distribution:\n",
      "language\n",
      "Python    333\n",
      "Java      333\n",
      "C++       333\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "queries = get_queries_from_user()\n",
    "\n",
    "if queries == 0:\n",
    "    print(\"Using default queries.\")\n",
    "    queries = [\n",
    "        \"language:python stars:>100\",\n",
    "        \"language:java stars:>100\", \n",
    "        \"language:cpp stars:>100\"\n",
    "        # \"language:javascript stars:>100\",\n",
    "        # \"language:go stars:>50\",\n",
    "        # \"language:rust stars:>50\",\n",
    "        # \"language:php stars:>50\",\n",
    "    ]\n",
    "    \n",
    "all_repositories = []\n",
    "\n",
    "for i, query in enumerate(queries):\n",
    "    print(f\"\\nSearch {i+1}/{len(queries)}: {query}\")\n",
    "    \n",
    "    repos = search_repositories(query, max_repos=MAX_REPOS // len(queries))\n",
    "    all_repositories.extend(repos)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    seen_ids = set()\n",
    "    unique_repos = []\n",
    "    for repo in all_repositories:\n",
    "        if repo['id'] not in seen_ids:\n",
    "            seen_ids.add(repo['id'])\n",
    "            unique_repos.append(repo)\n",
    "    all_repositories = unique_repos\n",
    "    \n",
    "    print(f\"Total unique repositories: {len(all_repositories)}\")\n",
    "    time.sleep(5)\n",
    "\n",
    "if all_repositories:\n",
    "    save_repo_list(all_repositories, OUTPUT_FILE)\n",
    "    print(f\"\\nScraping complete! {len(all_repositories)} repositories saved.\")\n",
    "    \n",
    "    # Display summary\n",
    "    df = pd.DataFrame(all_repositories)\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"Languages found: {df['language'].nunique()}\")\n",
    "    print(f\"Total stars: {df['stargazers_count'].sum():,}\")\n",
    "    print(f\"Total forks: {df['forks_count'].sum():,}\")\n",
    "    print(f\"\\nLanguage distribution:\")\n",
    "    print(df['language'].value_counts().head(10))\n",
    "else:\n",
    "    print(\"No repositories collected\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
