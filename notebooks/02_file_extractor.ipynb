{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e19820c",
   "metadata": {},
   "source": [
    "# File Extractor - Web Scraping Version\n",
    "## Extract relevant source code files directly from GitHub URLs\n",
    "\n",
    "This notebook web scrapes source code directly from GitHub repositories using the URLs generated by the scraper, without cloning repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e536a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c7fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "REPOS_FILE = \"../../data/repo_details/repository_list_scrap_list.json\"\n",
    "OUTPUT_DIR = \"../../data/extracted_files\"\n",
    "MAX_FILES_PER_REPO = 50\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "})\n",
    "\n",
    "request_count = 0\n",
    "last_request_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dba5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(url):\n",
    "    global request_count, last_request_time\n",
    "    \n",
    "    current_time = time.time()\n",
    "    if current_time - last_request_time < 1.0:\n",
    "        time.sleep(1.0 - (current_time - last_request_time))\n",
    "        \n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        request_count += 1\n",
    "        last_request_time = time.time()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response\n",
    "        elif response.status_code == 429:\n",
    "            print(\"Rate limited. Waiting 30 seconds...\")\n",
    "            time.sleep(30)\n",
    "            return make_request(url)\n",
    "        else:\n",
    "            return None \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19beba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language_from_path(file_path):\n",
    "    file_path_lower = file_path.lower()\n",
    "    \n",
    "    source_extensions = {\n",
    "        '.py': 'python',\n",
    "        '.java': 'java', \n",
    "        '.cpp': 'cpp', '.cc': 'cpp', '.cxx': 'cpp', '.hpp': 'cpp',\n",
    "    }\n",
    "    \n",
    "    for ext, lang in source_extensions.items():\n",
    "        if file_path_lower.endswith(ext):\n",
    "            return lang\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa07fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_branch(repo_url):\n",
    "    response = make_request(repo_url)\n",
    "    \n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        branch_select = soup.find('details', {'data-test-selector': 'branch-select-menu'})\n",
    "        if branch_select:\n",
    "            branch_name = branch_select.find('span', class_='css-truncate-target')\n",
    "            if branch_name:\n",
    "                return branch_name.text.strip()\n",
    "    \n",
    "    for branch in ['main', 'master']:\n",
    "        test_url = f\"{repo_url}/tree/{branch}\"\n",
    "        if make_request(test_url):\n",
    "            return branch\n",
    "    \n",
    "    return 'main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c12531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repository_tree(repo_url, branch, path_in_repo=\"\", all_files=None, depth=0, max_depth=10):\n",
    "    if all_files is None:\n",
    "        all_files = []\n",
    "    \n",
    "    if depth > max_depth:\n",
    "        return all_files\n",
    "    \n",
    "    if path_in_repo:\n",
    "        url = f\"{repo_url}/tree/{branch}/{path_in_repo}\"\n",
    "    else:\n",
    "        url = f\"{repo_url}/tree/{branch}\"\n",
    "        \n",
    "    response = make_request(url)\n",
    "    if not response:\n",
    "        return all_files\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    file_containers = [\n",
    "        soup.find('div', {'role': 'grid'}),\n",
    "        soup.find('table'),\n",
    "        soup.find('div', class_='Box'),\n",
    "        soup.find('div', class_='js-navigation-container'),\n",
    "    ]\n",
    "    \n",
    "    file_container = next((container for container in file_containers if container is not None), None)\n",
    "    \n",
    "    if not file_container:\n",
    "        links = soup.find_all('a', href=True)\n",
    "    else:\n",
    "        links = file_container.find_all('a', href=True)\n",
    "    \n",
    "    directories_to_scan = []\n",
    "    \n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        text = link.text.strip()\n",
    "        \n",
    "        if not text or text == '..':\n",
    "            continue\n",
    "        \n",
    "        if any(skip in href for skip in ['/commits/', '/blame/', '/graphs/']):\n",
    "            continue\n",
    "        \n",
    "        # Directory detection\n",
    "        if f'/tree/{branch}' in href and not href.endswith(f'/tree/{branch}'):\n",
    "            path_parts = href.split(f'/tree/{branch}/')\n",
    "            if len(path_parts) > 1:\n",
    "                dir_path = path_parts[1]\n",
    "                if dir_path and dir_path not in directories_to_scan:\n",
    "                    directories_to_scan.append(dir_path)\n",
    "        \n",
    "        # File detection\n",
    "        elif f'/blob/{branch}' in href:\n",
    "            path_parts = href.split(f'/blob/{branch}/')\n",
    "            if len(path_parts) > 1:\n",
    "                file_path = path_parts[1]\n",
    "                if file_path and file_path not in all_files:\n",
    "                    all_files.append(file_path)\n",
    "    \n",
    "    # Recursively scan directories\n",
    "    for dir_path in directories_to_scan:\n",
    "        skip_dirs = ['.git', 'node_modules', '__pycache__', 'build', 'dist', 'target', 'bin', 'obj', 'vendor']\n",
    "        if any(skip_dir in dir_path.split('/') for skip_dir in skip_dirs):\n",
    "            continue\n",
    "            \n",
    "        get_repository_tree(repo_url, branch, dir_path, all_files, depth + 1, max_depth)\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78a5c724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_content(repo_url, file_path, branch):\n",
    "    # Raw URL - primary method\n",
    "    raw_url = repo_url.replace('https://github.com/', 'https://raw.githubusercontent.com/')\n",
    "    raw_url = f\"{raw_url}/{branch}/{file_path}\"\n",
    "    \n",
    "    response = make_request(raw_url)\n",
    "    if response and response.status_code == 200:\n",
    "        content = response.text\n",
    "        if content and content.strip():\n",
    "            return content, True\n",
    "    \n",
    "    return \"\", False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2df9a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_repository(repo_data, max_files=MAX_FILES_PER_REPO):\n",
    "    full_name = repo_data['full_name']\n",
    "    repo_url = repo_data['clone_url'].replace('.git', '')\n",
    "    primary_language = repo_data['language']\n",
    "    \n",
    "    print(f\"\\nProcessing: {full_name} ({primary_language})\")\n",
    "    \n",
    "    branch = get_default_branch(repo_url)\n",
    "    print(f\"   Branch: {branch}\")\n",
    "    \n",
    "    all_files = get_repository_tree(repo_url, branch)\n",
    "    print(f\"   Found {len(all_files)} total files\")\n",
    "    \n",
    "    if not all_files:\n",
    "        return []\n",
    "    \n",
    "    # Filter for source code files\n",
    "    source_files = []\n",
    "    for file_path in all_files:\n",
    "        if any(skip_dir in file_path for skip_dir in [\n",
    "            '.git/', 'node_modules/', '__pycache__/', 'build/', 'dist/', \n",
    "            'target/', 'bin/', 'obj/', 'vendor/', '.github/'\n",
    "        ]):\n",
    "            continue\n",
    "        \n",
    "        language = detect_language_from_path(file_path)\n",
    "        if language:\n",
    "            source_files.append({\n",
    "                'path': file_path,\n",
    "                'language': language\n",
    "            })\n",
    "    \n",
    "    print(f\"   Identified {len(source_files)} source code files\")\n",
    "    \n",
    "    if not source_files:\n",
    "        return []\n",
    "    \n",
    "    # Prioritize and limit files if needed\n",
    "    if len(source_files) > max_files:\n",
    "        print(f\"   Limiting to {max_files} files\")\n",
    "        \n",
    "        def priority_score(file_path):\n",
    "            path_lower = file_path.lower()\n",
    "            score = 0\n",
    "            if any(src_dir in path_lower for src_dir in ['src/', 'lib/', 'app/', 'source/', 'main/']):\n",
    "                score += 3\n",
    "            elif '/' not in file_path or file_path.count('/') == 0:\n",
    "                score += 1\n",
    "            if 'test' in path_lower:\n",
    "                score -= 1\n",
    "            return score\n",
    "        \n",
    "        source_files.sort(key=lambda x: (priority_score(x['path']), -len(x['path'])), reverse=True)\n",
    "        source_files = source_files[:max_files]\n",
    "    \n",
    "    # Download file contents\n",
    "    extracted_files = []\n",
    "    print(f\"   Downloading content...\")\n",
    "    \n",
    "    for i, file_info in enumerate(source_files, 1):\n",
    "        file_path = file_info['path']\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            print(f\"   Progress: {i}/{len(source_files)}\")\n",
    "        \n",
    "        content, success = download_file_content(repo_url, file_path, branch)\n",
    "        \n",
    "        if success and content and 10 <= len(content) <= 100000:\n",
    "            extracted_files.append({\n",
    "                'repo_name': full_name,\n",
    "                'repo_url': repo_url,\n",
    "                'repo_language': primary_language,\n",
    "                'file_path': file_path,\n",
    "                'file_language': file_info['language'],\n",
    "                'content': content,\n",
    "                'size': len(content),\n",
    "                'lines': len(content.splitlines())\n",
    "            })\n",
    "            print(f\"   ✅ {file_path}\")\n",
    "        else:\n",
    "            print(f\"   ❌ {file_path}\")\n",
    "        \n",
    "        time.sleep(1.0)\n",
    "    \n",
    "    print(f\"   Successfully extracted {len(extracted_files)} files\")\n",
    "    return extracted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a62fd8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(all_files, output_dir):\n",
    "    output_file = os.path.join(output_dir, \"all_extracted_files.json\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_files, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Group files by repository\n",
    "    files_by_repo = {}\n",
    "    for file_data in all_files:\n",
    "        repo_name = file_data['repo_name']\n",
    "        files_by_repo.setdefault(repo_name, []).append(file_data)\n",
    "    \n",
    "    # Save files grouped by repository\n",
    "    repo_files_dir = os.path.join(output_dir, \"by_repo\")\n",
    "    os.makedirs(repo_files_dir, exist_ok=True)\n",
    "    \n",
    "    for repo_name, files in files_by_repo.items():\n",
    "        safe_name = repo_name.replace('/', '_')\n",
    "        repo_file = os.path.join(repo_files_dir, f\"{safe_name}_files.json\")\n",
    "        with open(repo_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(files, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Create summary\n",
    "    summary = {\n",
    "        'total_files': len(all_files),\n",
    "        'total_repos': len(files_by_repo),\n",
    "        'total_requests': request_count,\n",
    "        'files_per_repo': {repo: len(files) for repo, files in files_by_repo.items()},\n",
    "        'file_languages': {}\n",
    "    }\n",
    "    \n",
    "    for file_data in all_files:\n",
    "        summary['file_languages'][file_data['file_language']] = summary['file_languages'].get(file_data['file_language'], 0) + 1\n",
    "    \n",
    "    summary_file = os.path.join(output_dir, \"extraction_summary.json\")\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"All files saved to {output_dir}\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "516d7728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 999 repositories\n",
      "Starting extraction...\n",
      "\n",
      "==================================================\n",
      "[1/999] Cangol/AndroidStackBlur\n",
      "\n",
      "Processing: Cangol/AndroidStackBlur (Java)\n",
      "   Branch: master\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m50\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(repos_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo[\u001b[33m'\u001b[39m\u001b[33mfull_name\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m files = \u001b[43mprocess_repository\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m files:\n\u001b[32m     23\u001b[39m     all_extracted_files.extend(files)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mprocess_repository\u001b[39m\u001b[34m(repo_data, max_files)\u001b[39m\n\u001b[32m      8\u001b[39m branch = get_default_branch(repo_url)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Branch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbranch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m all_files = \u001b[43mget_repository_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbranch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m total files\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_files:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mget_repository_tree\u001b[39m\u001b[34m(repo_url, branch, path_in_repo, all_files, depth, max_depth)\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(skip_dir \u001b[38;5;129;01min\u001b[39;00m dir_path.split(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m skip_dir \u001b[38;5;129;01min\u001b[39;00m skip_dirs):\n\u001b[32m     65\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[43mget_repository_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbranch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     time.sleep(\u001b[32m0.5\u001b[39m)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m all_files\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mget_repository_tree\u001b[39m\u001b[34m(repo_url, branch, path_in_repo, all_files, depth, max_depth)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     11\u001b[39m     url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbranch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m response = \u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response:\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m all_files\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mmake_request\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m      4\u001b[39m current_time = time.time()\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_time - last_request_time < \u001b[32m1.0\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_time\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_request_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      9\u001b[39m     response = session.get(url, timeout=\u001b[32m10\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load repository data\n",
    "if not os.path.exists(REPOS_FILE):\n",
    "    print(f\"Repository file not found: {REPOS_FILE}\")\n",
    "    exit()\n",
    "\n",
    "with open(REPOS_FILE, 'r', encoding='utf-8') as f:\n",
    "    repos_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(repos_data)} repositories\")\n",
    "print(f\"Starting extraction...\")\n",
    "\n",
    "# Process all repositories\n",
    "all_extracted_files = []\n",
    "successful_repos = 0\n",
    "\n",
    "for i, repo in enumerate(repos_data, 1):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"[{i}/{len(repos_data)}] {repo['full_name']}\")\n",
    "    \n",
    "    files = process_repository(repo)\n",
    "    \n",
    "    if files:\n",
    "        all_extracted_files.extend(files)\n",
    "        successful_repos += 1\n",
    "        print(f\"✅ Successfully processed {repo['full_name']} - {len(files)} files\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to process {repo['full_name']}\")\n",
    "    \n",
    "    if i < len(repos_data):\n",
    "        time.sleep(3)\n",
    "\n",
    "# Save and display results\n",
    "if all_extracted_files:\n",
    "    summary = save_results(all_extracted_files, OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"\\nEXTRACTION COMPLETE!\")\n",
    "    print(f\"Successfully processed {successful_repos}/{len(repos_data)} repositories\")\n",
    "    print(f\"Extracted {len(all_extracted_files)} total files\")\n",
    "    print(f\"Made {request_count} web requests\")\n",
    "    \n",
    "    print(f\"\\nFile Language Distribution:\")\n",
    "    for lang, count in sorted(summary['file_languages'].items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        percentage = (count / summary['total_files']) * 100\n",
    "        print(f\"   {lang}: {count} files ({percentage:.1f}%)\")\n",
    "        \n",
    "else:\n",
    "    print(\"No files were extracted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
